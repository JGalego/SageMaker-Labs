{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Tour of SageMaker\n",
    "\n",
    "<div style=\"text-align: left\"><img src=\"images/a_tour_of_sagemaker.png\" alt=\"A Tour of SageMaker\" style=\"width: 300px;\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to **A Tour of SageMaker**! üòä\n",
    "\n",
    "In this notebook-shaped presentation, we'll show you **every** nook and cranny ‚öôÔ∏è of [Amazon SageMaker](https://aws.amazon.com/sagemaker/) - our end-to-end ML service:\n",
    "\n",
    "* How it supports **all** stages of the ML pipeline - from data collection and labeling to model deployment... and back ‚ôæÔ∏è\n",
    "* How it automates the entire ML workflow - welllll, almost... üëÄ looking at you [Amazon Augmented AI](https://aws.amazon.com/augmented-ai/) - and last but *certainly* not least\n",
    "* How to optimize everything for cost üí∞\n",
    "\n",
    "Ready to master the (not so) mystic art of running ML-powered apps with Amazon SageMaker? üßô\n",
    "\n",
    "Hope you enjoy the ride! üòâ\n",
    "\n",
    "<div style=\"text-align: left\"><img src=\"https://media.tenor.com/jNGGYr4g4xAAAAAM/benedict-cumberbatch-dr-strange.gif\"/></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* [Introduction](#Introduction)\n",
    "* [The Current State of AI/ML](#The-Current-State-of-AI/ML)\n",
    "* [AWS ML Stack: The Big Picture](#AWS-ML-Stack:-The-Big-Picture)\n",
    "* [Amazon SageMaker Overview](#Amazon-SageMaker-Overview)\n",
    "* [The Quick Tour](#The-Quick-Tour)\n",
    "    - [Prerequisites](#Prerequisites)\n",
    "    - [Data Preparation](#Data-Preparation)\n",
    "        - [Download and Explore the Dataset](#Download-and-Explore-the-Dataset)\n",
    "        - [Prepare and Upload Data](#Prepare-and-Upload-Data)\n",
    "    - [Model Deployment](#Model-Deployment)\n",
    "    - [Model Tuning](#Model-Tuning)\n",
    "    - [Clean Up](#Clean-Up)\n",
    "* [Data Preparation with Data Wrangler](#Data-Preparation-with-Data-Wrangler)\n",
    "* [Feature Engineering with Processing Jobs](#Feature-Engineering-with-Amazon-SageMaker-Processing)\n",
    "* [Deploying Models at Scale](#Deploying-Models-at-Scale)\n",
    "* [Coming Up](#Coming-Up) üÜï\n",
    "* [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Current State of AI/ML\n",
    "\n",
    "[üîù Back to the top](#A-Tour-of-SageMaker)\n",
    "\n",
    "If you made it all the way here, you've probably heard these before, or some variation thereof:\n",
    "\n",
    "> \"Data is the new Oil\" üõ¢Ô∏è -- Clive Humby\n",
    "\n",
    "> \"AI is the new electricity\" ‚ö° -- Andrew Ng\n",
    "\n",
    "> \"ML is the last invention that humanity will ever need to make.\" üîÆ -- Nick Bostrom\n",
    "\n",
    "Riding [Gartner's Hype Cycle for AI](https://www.gartner.com/en/articles/what-s-new-in-artificial-intelligence-from-the-2022-gartner-hype-cycle), we can clearly see that while there's plenty of hype in and around AI/ML, there's also a lot of potential.\n",
    "\n",
    "<img src=\"https://emtemp.gcom.cloud/ngw/globalassets/en/articles/images/hype-cycle-for-artificial-intelligence-2022.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "Looking at the current state of the so-called digital economy, there's one thing we know for sure - the reach of ML is growing. üìà\n",
    "\n",
    "ü§® **Don't believe me?**\n",
    "\n",
    "Let's look at the numbers then:\n",
    "\n",
    "* By 2025, global spending on AI will reach $204 billion (source: [IDC](https://www.idc.com/getdoc.jsp?containerId=prUS48191221))\n",
    "* By the end of 2024, 75% of enterprises will shift from piloting to operationalizing AI (source: [Gartner](https://www.gartner.com/en/newsroom/press-releases/2020-06-22-gartner-identifies-top-10-data-and-analytics-technolo))\n",
    "* 57% say that AI would transform¬†their organization in¬†the¬†next 3 years (source: [Deloitte](https://www2.deloitte.com/us/en/insights/focus/cognitive-technologies/state-of-ai-and-intelligent-automation-in-business-survey.html))\n",
    "\n",
    "\"Well, well, well\", you might say, \"this looks promising! Maybe I should jump on the [bandwagon](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1056774) after all\"... \n",
    "\n",
    "![But that's not the whole story](https://c.tenor.com/hsk-j1UTNFMAAAAC/but-thats-not-the-whole-story-derek-muller.gif)\n",
    "\n",
    "Here's the scary part (source: [InfoWorld](https://www.infoworld.com/article/3639028/why-ai-investments-fail-to-deliver.html)):\n",
    "\n",
    "* 85% of all AI/ML projects fail to deliver üò®\n",
    "* While ~50% never make it to production üò±\n",
    "\n",
    "‚ö†Ô∏è **What's happening there? Why is the failure rate so high?**\n",
    "\n",
    "The truth is that ML development can be both **complex** ü§Ø and **costly** üí∏\n",
    "\n",
    "Traditionally, there are multiple barriers to adoption at every step of the ML workflow:\n",
    "* Data collection and preparation can be **time consuming** and **undifferentiated**\n",
    "* Choosing the right ML algorithm is often done by **trial and error**\n",
    "* Lenghty training times often lead to **higher costs**\n",
    "* Model tuning can involve **very long cycles** and require adjusting thousands of different combinations\n",
    "* Models need to be **monitored constantly** and **scaled** to meet demand\n",
    "\n",
    "To make matters worse, many of the tools developers take for granted when building traditional software such as debuggers, project management, collaboration, and so forth are disconnected when it comes to ML development. \n",
    "\n",
    "**How can we make things simpler?**\n",
    "\n",
    "Enter [Amazon SageMaker](https://aws.amazon.com/sagemaker/)...\n",
    "\n",
    "![Et Voil√°!](https://media.tenor.com/NWqisN5ga_MAAAAC/voila-iron-man.gif)\n",
    "\n",
    "Amazon SageMaker was built from the ground up to provide every developer and data scientist the ability to build, train, and deploy ML models quickly and at lower cost by providing the tools required for every step of the ML development lifecycle in one integrated, fully managed service.\n",
    "\n",
    "But before we dive deeper into how Amazon SageMaker does this, let's take a step back and look at what AWS has to offer in terms of AI/ML and where Amazon SageMaker fits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS ML Stack: The Big Picture\n",
    "\n",
    "[üîù Back to the top](#A-Tour-of-SageMaker)\n",
    "\n",
    "At AWS, our mission is to\n",
    "\n",
    "> **Put ML in the hands of every developer and data scientist**.\n",
    "\n",
    "We're constantly creating and innovating on behalf of our customers to deliver the broadest and deepest set of ML capabilities for builders of all levels of expertise. \n",
    "\n",
    "The result of this combined effort is what we call the **AWS ML Stack** (*pictured below*).\n",
    "\n",
    "<img src=\"images/aws_ml_stack.png\" alt=\"AWS ML Stack\" style=\"width: 900px;\"/>\n",
    "\n",
    "Each layer of the stack is focused on removing what we call [undifferentiated heavy lifting](https://aws.amazon.com/blogs/aws/we_build_muck_s/) i.e. work that adds little or no value to the mission of a company, so that our customers can move faster.\n",
    "\n",
    "**Let's look at each layer more closely...**\n",
    "\n",
    "At the **ML Frameworks & Infrastructure** layer (*bottom*), expert practitioners can develop on the framework of their choice as a managed experience in Amazon SageMaker or use the [AWS Deep Learning AMIs](https://aws.amazon.com/machine-learning/amis/), which are fully configured with the latest versions of the most popular deep learning frameworks and tools ‚Äì including [PyTorch](https://aws.amazon.com/pytorch/), [MXNet](https://aws.amazon.com/mxnet/), TensorFlow, and [many more](https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.html). AWS provides the broadest and deepest portfolio of compute, networking, and storage infrastructure services with a choice of processors and accelerators - [AWS Trainium / `Trn1` Instances](https://aws.amazon.com/machine-learning/trainium/) and [Habana Gaudi / `DL1` Instances](https://aws.amazon.com/ec2/instance-types/dl1/) for training, [AWS Inferentia / `Inf1` Instances](https://aws.amazon.com/machine-learning/inferentia/) for inference, and even [FPGAs / `F1` Instances](https://aws.amazon.com/ec2/instance-types/f1/) - to meet our customers' unique performance and budget needs for ML.\n",
    "\n",
    "> üìö For an overview of the different services that support ML workloads in terms of infrastructure, check out [AWS Machine Learning Infrastructure](https://aws.amazon.com/machine-learning/infrastructure/)\n",
    "\n",
    "<img src=\"images/ml_infra_overview.png\" alt=\"ML Infrastructure Overview\" style=\"width: 900px;\"/>\n",
    "\n",
    "At the **ML Services** layer (*middle*) is Amazon SageMaker, which provides every developer and data scientist with the ability to build, train, and deploy ML models *at scale*. It removes the complexity from each step of the ML workflow so you can more easily deploy your ML use cases, anything from predictive maintenance to computer vision to predicting customer behaviors. Customers achieve up to 10x improvement in data scientists' productivity with Amazon SageMaker.\n",
    "\n",
    "Finally, the **AI Services** layer (*top*) contains services that allow developers to easily add intelligence to any application **without** needing ML skills. We group these services into two sub-groups: **Core** services include text, documents, chatbots, speech and vision, while **Specialized** services contains everything related to Business Processes, Search, Code & DevOps, Industrial and Heathcare.\n",
    "\n",
    "> üí° Want to see some of these AI services in action? Head over to the [AWS AI Service Demos](https://ai-service-demos.go-aws.com/)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1358/0*MNUhgCBYMi5k849I\" alt=\"ML Infrastructure Overview\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon SageMaker Overview\n",
    "\n",
    "[üîù Back to the top](#A-Tour-of-SageMaker)\n",
    "\n",
    "SageMaker is a big service with a lot of different features and capabilities (*pictured below*).\n",
    "\n",
    "We typically talk about those capabilities as falling into four categories:\n",
    "\n",
    "1. Data Preparation\n",
    "2. Model Building\n",
    "3. Training & Tuning, and\n",
    "4. Deployment & Management\n",
    "\n",
    "These four sets of capabilities address the needs that ML builders have and the challenges they face at each stage of a model's lifecycle.\n",
    "\n",
    "**Let's do a quick (mental) exercise...**\n",
    "\n",
    "Look carefully at the image below üëÄ Read each feature description - don't just F- or Z-scan your way through.\n",
    "\n",
    "**Is there anything that may be of particular interest to you and your organization at this point in time?**\n",
    "\n",
    "<img src=\"images/sagemaker_overview.png\" alt=\"ML Infrastructure Overview\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Quick Tour\n",
    "\n",
    "[üîù Back to the top](#A-Tour-of-SageMaker)\n",
    "\n",
    "Now that we know what Amazon SageMaker is and what it has to offer, let's look at a simple use case that showcases a small subset of its capabilities.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "[üîù Back to the top](#A-Tour-of-SageMaker)\n",
    "\n",
    "We'll start by importing some Python libraries and defining some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import boto3                         # AWS SDK for Python\n",
    "import sagemaker                     # Amazon SageMaker SDK for Python\n",
    "\n",
    "import numpy as np                   # Matrix multiplication and numerical processing\n",
    "import pandas as pd                  # Munging tabular data\n",
    "import matplotlib.pyplot as plt      # Charts and visualizations\n",
    "\n",
    "from IPython.display import (        # Display tools in IPython\n",
    "    display,\n",
    "    HTML,\n",
    "    Image,\n",
    "    Latex,\n",
    "    Markdown\n",
    ")\n",
    "\n",
    "def printmd(str):\n",
    "    \"\"\"Prints a Markdown string\"\"\"\n",
    "    display(Markdown(str))\n",
    "\n",
    "def printhtml(str):\n",
    "    \"\"\"Prints an HTML string\"\"\"\n",
    "    return display(HTML(str))\n",
    "\n",
    "def printtex(str):\n",
    "    \"\"\"Prints a Latex string\"\"\"\n",
    "    return display(Latex(str))\n",
    "\n",
    "# Debug\n",
    "printmd(f\"Numpy: `{np.__version__}`\")\n",
    "printmd(f\"Pandas: `{pd.__version__}`\")\n",
    "printmd(f\"Boto3: `{boto3.__version__}`\")\n",
    "printmd(f\"SageMaker: `{sagemaker.__version__}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's gather some constants that we'll use later in this demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SageMaker client\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "# Manages interactions with the Amazon SageMaker APIs\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "\n",
    "# The AWS Region that we're using\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# The IAM execution role assumed by SageMaker\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# The S3 bucket to be used by this session\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# Where we'll store our data and model artifacts\n",
    "prefix = \"smlabs/sagemaker_tour\"\n",
    "\n",
    "printmd(f\"Region üåé: `{region}`\")\n",
    "printmd(f\"Bucket ü™£: `{bucket}`\")\n",
    "printmd(f\"Role üë∑: `{role}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "[üîù Back to the top](#A-Tour-of-SageMaker)\n",
    "\n",
    "> This section was adapted from [Download, Prepare and Upload Training Data](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-ex-data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Explore the Dataset\n",
    "\n",
    "[üîù Back to the top](#A-Tour-of-SageMaker)\n",
    "\n",
    "In this demo, we'll use the [Bank Marketing Data Set](https://archive.ics.uci.edu/ml/datasets/bank+marketing) (Moro *et al.*, 2014).\n",
    "\n",
    "The data is comes from direct marketing campaigns of a Portuguese banking institution üè¶\n",
    "\n",
    "The marketing campaigns were based on phone calls üìû\n",
    "\n",
    "Often, more than one contact to the same client was required in order to access if the product (bank term deposit) would be subscribed (`yes`) üëç or not (`no`) üëé\n",
    "\n",
    "There are four datasets:\n",
    "\n",
    "1. `bank-additional-full.csv` with all examples (41188) and 20 inputs, ordered by date (from May 2008 to November 2010)\n",
    "\n",
    "2. `bank-additional.csv` with 10% of the examples (4119), randomly selected from 1), and 20 inputs.\n",
    "\n",
    "3. `bank-full.csv` with all examples and 17 inputs, ordered by date (older version of this dataset with less inputs).\n",
    "\n",
    "4. `bank.csv` with 10% of the examples and 17 inputs, randomly selected from 3 (older version of this dataset with less inputs).\n",
    "\n",
    "The goal is to predict whether the client will subscribe (`yes/no`) to a term deposit as indicated by the variable `y`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by downloading the dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -N \"https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\"\n",
    "!unzip -o bank-additional.zip\n",
    "# This will create a folder named 'bank-additional'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this demo, we'll use the full dataset with all the inputs (`bank-additional-full.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from CSV file\n",
    "data = pd.read_csv('./bank-additional/bank-additional-full.csv')\n",
    "pd.set_option('display.max_columns', 500)    # Make sure we can see all columns\n",
    "pd.set_option('display.max_rows', 50)        # Make sure we can see all rows\n",
    "\n",
    "# Debug\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which we'll store in the default bucket for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_source = sagemaker_session.upload_data('./bank-additional/bank-additional-full.csv', bucket=bucket, key_prefix=f'{prefix}/input_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's talk a little bit about the data and its attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat bank-additional/bank-additional-names.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start asking some questions...\n",
    "\n",
    "**1/ How are the features distributed?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency tables for each categorical feature\n",
    "printmd(\"##### Categorical Features \")\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=data[column], columns='% observations', normalize='columns'))\n",
    "\n",
    "# Histograms for each numeric features\n",
    "printmd(\"##### Numeric Features \")\n",
    "display(data.describe())\n",
    "%matplotlib inline\n",
    "hist = data.hist(bins=30, sharey=True, figsize=(12, 12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2/ How are the features related to one another?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(data, figsize=(12, 12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What conclusions can you draw from the information above?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and Upload Data\n",
    "\n",
    "[üîù Back to the top](#A-Tour-of-SageMaker)\n",
    "\n",
    "Our preprocessing pipeline will be very simple.\n",
    "\n",
    "We will create a variable to indicate that there was no prior contact (`no_previous_contact`) and another one to indicate whether the individual is currently employed (`not_working`).\n",
    "\n",
    "Finally, we will convert all categorical variables into dummy/indicator variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['no_previous_contact'] = np.where(data['pdays'] == 999, 1, 0)                                 # Indicator variable to capture when pdays takes a value of 999 (no prior contact)\n",
    "data['not_working'] = np.where(np.in1d(data['job'], ['student', 'retired', 'unemployed']), 1, 0)   # Indicator for individuals not actively employed\n",
    "model_data = pd.get_dummies(data)                                                                  # Convert categorical variables into sets of indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also remove all the economic features (see description above) and `duration` from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data.drop([\n",
    "    'duration',\n",
    "    'emp.var.rate',\n",
    "    'cons.price.idx',\n",
    "    'cons.conf.idx',\n",
    "    'euribor3m',\n",
    "    'nr.employed'\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare and Upload Data\n",
    "\n",
    "We'll split the data into three datasets: **train**, **validation**, **test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sort the data then split out first 70%, second 20%, and last 10%\n",
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=42), [int(0.7 * len(model_data)), int(0.9 * len(model_data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we'll use the [built-in XGBost](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html) container which supports CSV and libsvm formats for training and inference.\n",
    "\n",
    "We'll keep the original CSV format, but there are a few restrictions:\n",
    "\n",
    "1. The first column must be the target variable and\n",
    "2. The CSV should **not** include headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\n",
    "pd.concat([validation_data['y_yes'], validation_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we upload a copy of the data to the default S3 bucket where SageMaker can access it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_source = sagemaker_session.upload_data('train.csv', bucket=bucket, key_prefix=f'{prefix}/train')\n",
    "validation_source = sagemaker_session.upload_data('validation.csv', bucket=bucket, key_prefix=f'{prefix}/validation')\n",
    "\n",
    "# Debug\n",
    "print(train_source)\n",
    "!aws s3 ls {bucket}/{prefix}/train/\n",
    "print(validation_source)\n",
    "!aws s3 ls {bucket}/{prefix}/validation/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "[üîù Back to the top](#A-Tour-of-SageMaker)\n",
    "\n",
    "As we mentioned in the previous section, we'll use [XGBoost](https://github.com/dmlc/xgboost) to predict whether an individual will subscribe to the product. **But what exactly is it?**\n",
    "\n",
    "XGBoost is a popular and efficient open-source implementation of the gradient boosted trees algorithm - a supervised learning algorithm that attempts to accurately predict a target variable by combining the estimates of a set of simpler, weaker models.\n",
    "\n",
    "It is a widely used tool among Kaggle competitors and can usually be found in many winning submissions.\n",
    "\n",
    "> For additional details, check out [How XGBoost Works](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost-HowItWorks.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need is the location of the image that contains SageMaker's implementation of XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve(region=boto3.Session().region_name, framework='xgboost', version='latest')\n",
    "printmd(f\"XGBoost Image üì¶: `{container}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create input channels for the train and validation datasets that we upload to the S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset paths\n",
    "train_path = f\"s3://{bucket}/{prefix}/train/\"\n",
    "validation_path = f\"s3://{bucket}/{prefix}/validation/\"\n",
    "test_path = f\"s3://{bucket}/{prefix}/test/\"\n",
    "output_path = f\"s3://{bucket}/{prefix}/output/\"\n",
    "\n",
    "# Declare inputs\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_path, content_type='csv')\n",
    "s3_input_validation = sagemaker.inputs.TrainingInput(s3_data=validation_path, content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can start training our model\n",
    "\n",
    "> Wondering which instance types are available? Head over to [Amazon SageMaker Pricing](https://aws.amazon.com/sagemaker/pricing/), scroll down to the **On-Demand Pricing** section, select the tab that matches your use case and the AWS region you're in for a list of supported instance types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create estimator\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m5.large',\n",
    "                                    output_path=output_path,\n",
    "                                    sagemaker_session=sagemaker_session)\n",
    "\n",
    "# 2. Set hyperparameters\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)\n",
    "\n",
    "# 3. Start training\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deployment\n",
    "\n",
    "[üîù Back to the top](#A-Tour-of-SageMaker)\n",
    "\n",
    "Now that the training phase is finished, we can deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count=1,\n",
    "                           instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "\n",
    "[üîù Back to the top](#A-Tour-of-SageMaker)\n",
    "\n",
    "One easy way to evaluate the model is compare actual vs predicted values.\n",
    "\n",
    "In our case, since we're simply trying to predict whether the customer subscribes to a product (`1`) or not (`0`), this will produce a standard 2-class confusion matrix.\n",
    "\n",
    "To pass data between our endpoint, we'll serialize it as a CSV string and decode the resulting CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.serializer = sagemaker.serializers.CSVSerializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a simple function to make the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, predictor, rows=100):\n",
    "    \"\"\"Returns predictions for a dataset by invoking a predictor endpoint\"\"\"\n",
    "    # Split dataset into mini-batches of rows\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, predictor.predict(array).decode('utf-8')])\n",
    "    return np.fromstring(predictions[1:], sep=',')  # drop the target variable from the dataset\n",
    "\n",
    "predictions = predict(test_data.drop(['y_no', 'y_yes'], axis=1).to_numpy(), xgb_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what the confusion matrix looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(index=test_data['y_yes'], columns=np.round(predictions), rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but... **can we do better?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning\n",
    "\n",
    "[üîù Back to the top](#A-Tour-of-SageMaker)\n",
    "\n",
    "[Amazon SageMaker automatic model tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html), also known as hyperparameter tuning, finds the best version of a model by running many training jobs on your dataset using the algorithm and ranges of hyperparameters that you specify. \n",
    "\n",
    "It then selects the hyperparameter values that result in a model that performs the best, as measured by a metric that you chose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import (\n",
    "    ContinuousParameter,\n",
    "    IntegerParameter,\n",
    "    HyperparameterTuner\n",
    ")\n",
    "\n",
    "# 1. Define hyperparameter ranges\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html\n",
    "hyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n",
    "                         'min_child_weight': ContinuousParameter(1, 10),\n",
    "                         'alpha': ContinuousParameter(0, 2),\n",
    "                         'max_depth': IntegerParameter(1, 10)}\n",
    "\n",
    "# 2. Define objective metric\n",
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-metrics.html\n",
    "objective_metric_name = 'validation:auc'\n",
    "\n",
    "# 3. Initialize hyperparameter tuner\n",
    "tuner = HyperparameterTuner(xgb,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=20,\n",
    "                            max_parallel_jobs=3)\n",
    "\n",
    "# 4. Start model tuning\n",
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return the best training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.best_training_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_predictor = tuner.deploy(initial_instance_count=1,\n",
    "                               instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let's look at the new confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "predictions = predict(test_data.drop(['y_no', 'y_yes'], axis=1).to_numpy(), tuner_predictor)\n",
    "pd.crosstab(index=test_data['y_yes'], columns=np.round(predictions), rownames=['actuals'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up\n",
    "\n",
    "[üîù Back to the top](#A-Tour-of-SageMaker)\n",
    "\n",
    "Don't forget to remove the hosted endpoints or you'll start accruing costs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "tuner_predictor.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation with Data Wrangler\n",
    "\n",
    "[üîù Back to the top](#A-Tour-of-SageMaker)\n",
    "\n",
    "[SageMaker Data Wrangler](https://aws.amazon.com/sagemaker/data-wrangler) is a feature of Amazon SageMaker Studio that provides an end-to-end solution to import, prepare, transform, featurize, and analyze data. \n",
    "\n",
    "You can integrate a Data Wrangler data preparation flow into your ML workflows to simplify and streamline data pre-processing and feature engineering using little to no coding. \n",
    "\n",
    "You can also add your own Python scripts and transformations to customize workflows.\n",
    "\n",
    "Data Wrangler provides the following core functionalities to help you analyze and prepare data for machine learning applications.\n",
    "\n",
    "* **Import** ‚Äì Connect to and import data from Amazon Simple Storage Service (Amazon S3), Amazon Athena (Athena), Amazon Redshift, Snowflake, and Databricks.\n",
    "\n",
    "* **Data Flow** ‚Äì Create a data flow to define a series of ML data prep steps. You can use a flow to combine datasets from different data sources, identify the number and types of transformations you want to apply to datasets, and define a data prep workflow that can be integrated into an ML pipeline.\n",
    "\n",
    "* **Transform** ‚Äì Clean and transform your dataset using standard transforms like string, vector, and numeric data formatting tools. Featurize your data using transforms like text and date/time embedding and categorical encoding.\n",
    "\n",
    "* **Generate Data Insights** ‚Äì Automatically verify data quality and detect abnormalities in your data with Data Wrangler Data Insights and Quality Report.\n",
    "\n",
    "* **Analyze** ‚Äì Analyze features in your dataset at any point in your flow. Data Wrangler includes built-in data visualization tools like scatter plots and histograms, as well as data analysis tools like target leakage analysis and quick modeling to understand feature correlation.\n",
    "\n",
    "* **Export** ‚Äì Export your data preparation workflow to a different location. The following are example locations:\n",
    "\n",
    "    - Amazon Simple Storage Service (Amazon S3) bucket\n",
    "\n",
    "    - Amazon SageMaker Model Building Pipelines ‚Äì Use SageMaker Pipelines to automate model deployment. You can export the data that you've transformed directly to the pipelines.\n",
    "\n",
    "    - Amazon SageMaker Feature Store ‚Äì Store the features and their data in a centralized store.\n",
    "\n",
    "    - Python script ‚Äì Store the data and their transformations in a Python script for your custom workflows.\n",
    "\n",
    "üìö **Exercise:** Try to replicate the data preparation steps from [The Quick Tour](#The-Quick-Tour) section using Data Wrangler.\n",
    "\n",
    "<img src=\"images/data_wrangler_flow.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering with Amazon SageMaker Processing\n",
    "\n",
    "[üîù Back to the top](#A-Tour-of-SageMaker)\n",
    "\n",
    "[SageMaker Processing Jobs](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html) provide a simplified, managed experience on SageMaker to run data processing workloads like feature engineering, data validation, model evaluation, and model interpretation.\n",
    "\n",
    "SageMaker takes your script, copies the data from S3, and runs the processing container (either from built-in image or a custom image that you provide).\n",
    "\n",
    "In the end, the output will be stored in an S3 bucket that you specify.\n",
    "\n",
    "> Your input data **must** be stored in an Amazon S3 bucket. Alternatively, you can use [Amazon Athena](https://aws.amazon.com/athena) or [Amazon Redshift](https://aws.amazon.com/redshift/) as input sources.\n",
    "\n",
    "<img src=\"https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/Processing-1.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this works in practice by mimicking the preprocessing steps we did in [The Quick Tour](#The-Quick-Tour).\n",
    "\n",
    "First, we need a processing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\"\"\"\n",
    "Processing script for the Bank Marketing Data Set\n",
    "https://archive.ics.uci.edu/ml/datasets/bank+marketing\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parses command line arguments\"\"\"\n",
    "\n",
    "    # Initialize parser\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Define arguments\n",
    "    parser.add_argument(\n",
    "        '--filepath',\n",
    "        type=str,\n",
    "        default='/opt/ml/processing/input/'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--filename',\n",
    "        type=str,\n",
    "        default='bank-additional-full.csv'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--outputpath',\n",
    "        type=str,\n",
    "        default='/opt/ml/processing/output/'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--categorical_features',\n",
    "        type=str,\n",
    "        default='y, job, marital, education, default, housing, loan, contact, month, day_of_week, poutcome'  # pylint: disable=line-too-long\n",
    "    )\n",
    "\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entrypoint\"\"\"\n",
    "    # Process arguments\n",
    "    args, _ = parse_args()\n",
    "\n",
    "    # Load data\n",
    "    data = pd.read_csv(os.path.join(args.filepath, args.filename))\n",
    "\n",
    "    # Change the value . into _\n",
    "    data = data.replace(regex=r'\\.', value='_')\n",
    "    data = data.replace(regex=r'\\_$', value='')\n",
    "\n",
    "    # Create a variable to indicate that there was no prior contact (no_previous_contact) and\n",
    "    # another one to indicate whether the individual is currently employed (not_working).\n",
    "    data[\"no_previous_contact\"] = (data[\"pdays\"] == 999).astype(int)\n",
    "    data[\"not_working\"] = data[\"job\"].isin([\"student\", \"retired\", \"unemployed\"]).astype(int)\n",
    "\n",
    "    # Drop duration and economics features\n",
    "    data = data.drop([\n",
    "        'duration',\n",
    "        'emp.var.rate',\n",
    "        'cons.price.idx',\n",
    "        'cons.conf.idx',\n",
    "        'euribor3m',\n",
    "        'nr.employed'\n",
    "    ], axis=1)\n",
    "\n",
    "    # Encode categorical features\n",
    "    data = pd.get_dummies(data)\n",
    "\n",
    "    # Train, test, validation split\n",
    "    train_data, validation_data, test_data = np.split(  # pylint: disable=unbalanced-tuple-unpacking\n",
    "        data.sample(frac=1, random_state=42), [int(0.7 * len(data)), int(0.9 * len(data))])\n",
    "\n",
    "    # Prepare data for upload\n",
    "    ## Train\n",
    "    pd.concat([\n",
    "        train_data['y_yes'],\n",
    "        train_data.drop(['y_yes','y_no'], axis=1)\n",
    "    ], axis=1).to_csv(\n",
    "        os.path.join(args.outputpath, 'train/train.csv'), index=False, header=False\n",
    "    )\n",
    "    ## Validation\n",
    "    pd.concat([\n",
    "        validation_data['y_yes'],\n",
    "        validation_data.drop(['y_yes','y_no'], axis=1)\n",
    "    ], axis=1).to_csv(\n",
    "        os.path.join(args.outputpath, 'validation/validation.csv'), index=False, header=False)\n",
    "    ## Test\n",
    "    test_data['y_yes'].to_csv(\n",
    "        os.path.join(args.outputpath, 'test/test_y.csv'), index=False, header=False\n",
    "    )\n",
    "    test_data.drop(['y_yes','y_no'], axis=1).to_csv(\n",
    "        os.path.join(args.outputpath, 'test/test_x.csv'), index=False, header=False\n",
    "    )\n",
    "\n",
    "    print(\"Exiting processing job\")\n",
    "\n",
    "if __name__== '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use [scikit-learn Processor](https://docs.aws.amazon.com/sagemaker/latest/dg/use-scikit-learn-processing-container.html) to run our job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "# 1. Initialize processor\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1, \n",
    "    base_job_name='sm-tour-skprocessing'\n",
    ")\n",
    "\n",
    "# 2. Start processing job\n",
    "sklearn_processor.run(\n",
    "    code='preprocessing.py',\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=input_source, \n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "            s3_input_mode=\"File\",\n",
    "            s3_data_distribution_type=\"ShardedByS3Key\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train_data\", \n",
    "            source=\"/opt/ml/processing/output/train\",\n",
    "            destination=train_path,\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"validation_data\",\n",
    "            source=\"/opt/ml/processing/output/validation\",\n",
    "            destination=validation_path\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"test_data\",\n",
    "            source=\"/opt/ml/processing/output/test\",\n",
    "            destination=test_path\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "!aws s3 ls {train_path}\n",
    "!aws s3 ls {validation_path}\n",
    "!aws s3 ls {test_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying Models at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[üîù Back to the top](#A-Tour-of-SageMaker)\n",
    "\n",
    "After training a model, there are many ways to deploy it using SageMaker:\n",
    "\n",
    "* [Real-Time Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints.html) for persistent, real-time endpoints that make one prediction at a time\n",
    "\n",
    "    <img src=\"https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/multi-model-endpoints-diagram.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "* [Serverless Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html) for workloads that have idle periods between traffic spurts and can tolerate cold starts\n",
    "\n",
    "    <img src=\"https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2022/10/21/image001-9-1024x468.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "* [Asynchronous Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/async-inference.html) for requests with large payload sizes up to 1GB, long processing times, and near real-time latency requirements\n",
    "\n",
    "    <img src=\"https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/async-architecture.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "* [Batch Transform](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform.html) to get predictions for an entire dataset\n",
    "\n",
    "    <img src=\"https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/batch-transform-data-processing.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "**Which one should you choose?**\n",
    "\n",
    "In this section, we will use the [Amazon SageMaker Serverless Inference Benchmarking Toolkit](https://aws.amazon.com/blogs/machine-learning/introducing-the-amazon-sagemaker-serverless-inference-benchmarking-toolkit/) to test different endpoint configurations and pit the optimal one against a comparable real-time hosting instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by installing the benchmarking library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sm-serverless-benchmarking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.create_model(image_uri=container, role=role)\n",
    "model.create(instance_type=\"ml.m5.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start a benchmark run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sm_serverless_benchmarking.sagemaker_runner import run_as_sagemaker_job\n",
    "from sm_serverless_benchmarking.utils import convert_invoke_args_to_jsonl\n",
    "\n",
    "# 1. Provide a representative set of examples\n",
    "ser = sagemaker.serializers.CSVSerializer()\n",
    "bench_data = test_data.drop(['y_no', 'y_yes'], axis=1).to_numpy()\n",
    "random_samples = bench_data[np.random.choice(bench_data.shape[0], size=20, replace=False)]\n",
    "example_invoke_args = [\n",
    "    {\"Body\": ser.serialize(sample), \"ContentType\": \"text/csv\"} for sample in random_samples\n",
    "]\n",
    "example_args_file = convert_invoke_args_to_jsonl(example_invoke_args, output_path=\".\")\n",
    "\n",
    "# 2. Start benchmark run\n",
    "print(\"Starting benchmark run ‚è±Ô∏è\")\n",
    "bench_run = run_as_sagemaker_job(\n",
    "    role=role,\n",
    "    model_name=model.name,\n",
    "    invoke_args_examples_file=example_args_file,\n",
    "    stability_benchmark_invocations=2500,\n",
    "    concurrency_benchmark_invocations=2500,\n",
    ")\n",
    "\n",
    "# 3. Get results\n",
    "print(\"Waiting for Processing Job\", end='')\n",
    "while sagemaker_client.describe_processing_job(ProcessingJobName=bench_run.latest_job.job_name)['ProcessingJobStatus'] != \"Completed\":\n",
    "    print(\".\", end='')\n",
    "    time.sleep(30)\n",
    "printmd(\n",
    "    f\"\\nOutputs were uploaded to `{bench_run.latest_job.outputs[0].destination}`\"\n",
    ")\n",
    "bench_report = boto3.client('s3').get_object(Bucket=bucket, Key=f\"{bench_run.latest_job.name}/output/benchmark_outputs/benchmarking_report/benchmarking_report.html\")['Body'].read().decode(\"utf-8\") \n",
    "printhtml(bench_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to clean up everything afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coming Up\n",
    "\n",
    "[üîù Back to the top](#A-Tour-of-SageMaker)\n",
    "\n",
    "**This project is still under construction!** üöß\n",
    "\n",
    "We're accepting feature demands, bug reports and pull requests.\n",
    "\n",
    "Here's what you can expect in a not-so-distant future:\n",
    "\n",
    "* Training Large Models with [Distributed Training](https://docs.aws.amazon.com/sagemaker/latest/dg/distributed-training.html) and [Managed Spot Training](https://docs.aws.amazon.com/sagemaker/latest/dg/model-managed-spot-training.html)\n",
    "* Human in the Loop with [Augmented AI](https://aws.amazon.com/augmented-ai/)\n",
    "* MLOps with [SageMaker Pipelines](https://aws.amazon.com/sagemaker/pipelines/)\n",
    "* ... and much, much more\n",
    "\n",
    "See you soon! üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[üîù Back to the top](#A-Tour-of-SageMaker)\n",
    "\n",
    "<div style=\"text-align: left\"><img src=\"https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fbucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com%2Fpublic%2Fimages%2F90cb787d-c3a1-48c4-86d1-84e7456a949a_500x213.gif\"/></div>\n",
    "\n",
    "### Tutorials\n",
    "\n",
    "* [Build, Train, and Deploy a Machine Learning Model with Amazon SageMaker](https://aws.amazon.com/getting-started/hands-on/build-train-deploy-machine-learning-model-sagemaker/)\n",
    "* [Train and tune a deep learning model at scale](https://aws.amazon.com/getting-started/hands-on/train-tune-deep-learning-model-amazon-sagemaker/)\n",
    "\n",
    "### Learning\n",
    "\n",
    "* [Amazon SageMaker Technical Deep Dive Series](https://www.youtube.com/playlist?list=PLhr1KZpdzukcOr_6j_zmSrvYnLUtgqsZz)\n",
    "* [Dive into Deep Learning](https://www.d2l.ai/) ‚Äì an interactive, notebook-shaped DL book\n",
    "\n",
    "### Guides\n",
    "\n",
    "* [Deploy a Model in Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-deployment.html)\n",
    "* [Use Amazon SageMaker Built-in Algorithms or Pre-trained Models](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html)\n",
    "* [Buy and Sell Amazon SageMaker Algorithms and Models in AWS Marketplace](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-marketplace.html)\n",
    "* [Using SageMaker JumpStart Models](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-models.html)\n",
    "* [Using Your Own Algorithm or Model](https://docs.aws.amazon.com/sagemaker/latest/dg/docker-containers-notebooks.html)\n",
    "* [Using Amazon Augmented AI for Human Review](https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-use-augmented-ai-a2i-human-review-loops.html)\n",
    "\n",
    "### Code\n",
    "\n",
    "* [AWSome SageMaker](https://github.com/aws-samples/awesome-sagemaker) ‚Äì a curated list of references for Amazon SageMaker\n",
    "* [Amazon SageMaker Examples](https://github.com/aws/amazon-sagemaker-examples) ‚Äì these are automatically available when using SageMaker Notebook Instances\n",
    "* [Hugging Face Notebooks > SageMaker](https://github.com/huggingface/notebooks/tree/main/sagemaker) ‚Äì sample notebooks that demonstrate how to build, train and deploy [ü§ó Transformers](https://github.com/huggingface/transformers) with Amazon SageMaker\n",
    "* [Amazon Augmented AI Sample Notebooks](https://github.com/aws-samples/amazon-a2i-sample-jupyter-notebooks)\n",
    "* [Optimizing NLP models with Amazon EC2 `Inf1` instances in Amazon SageMaker](https://github.com/aws-samples/aws-inferentia-huggingface-workshop)\n",
    "\n",
    "### Infrastructure\n",
    "\n",
    "* [AWS ML Infrastructure](https://aws.amazon.com/machine-learning/infrastructure/) ‚Äì an overview of the different services that support ML-specific workloads\n",
    "* [How to choose the right GPU for DL](https://towardsdatascience.com/choosing-the-right-gpu-for-deep-learning-on-aws-d69c157d8c86) ‚Äì a must read\n",
    "* HW accelerators - [AWS Trainium / `Trn1` Instances](https://aws.amazon.com/machine-learning/trainium/) and [Habana Gaudi / `DL1` Instances](https://aws.amazon.com/ec2/instance-types/dl1/) for training, [AWS Inferentia / `Inf1` Instances](https://aws.amazon.com/machine-learning/inferentia/) for inference, and even [FPGAs / `F1` Instances](https://aws.amazon.com/ec2/instance-types/f1/)\n",
    "\n",
    "### Blogs\n",
    "\n",
    "* [Deploying ML models using SageMaker Serverless Inference (Preview)](https://aws.amazon.com/blogs/machine-learning/deploying-ml-models-using-sagemaker-serverless-inference-preview/)\n",
    "* [Host Hugging Face transformer models using Amazon SageMaker Serverless Inference](https://aws.amazon.com/blogs/machine-learning/host-hugging-face-transformer-models-using-amazon-sagemaker-serverless-inference/)\n",
    "* [Introducing the Amazon SageMaker Serverless Inference Benchmarking Toolkit](https://aws.amazon.com/blogs/machine-learning/introducing-the-amazon-sagemaker-serverless-inference-benchmarking-toolkit/)\n",
    "* [Bring your own model with Amazon SageMaker script mode](https://aws.amazon.com/blogs/machine-learning/bring-your-own-model-with-amazon-sagemaker-script-mode/)\n",
    "* [Speed up YOLOv4 inference to twice as fast on Amazon SageMaker](https://aws.amazon.com/blogs/machine-learning/speed-up-yolov4-inference-to-twice-as-fast-on-amazon-sagemaker/)\n",
    "* [Accelerate BERT inference with Hugging Face Transformers and AWS Inferentia](https://huggingface.co/blog/bert-inferentia-sagemaker)\n",
    "* [Build custom Amazon SageMaker PyTorch models for real-time handwriting text recognition](https://aws.amazon.com/blogs/machine-learning/build-custom-amazon-sagemaker-pytorch-models-for-real-time-handwriting-text-recognition/)\n",
    "* [Julien Simon‚Äôs substack](https://substack.com/profile/100614256-julien-simon) ‚Äì Chief Evangelist @ ü§ó, former Global Technical Evangelist (AI/ML) @ AWS\n",
    "\n",
    "### Frameworks\n",
    "\n",
    "* [TensorFlow on AWS](https://aws.amazon.com/tensorflow/)\n",
    "* [PyTorch on AWS](https://aws.amazon.com/pytorch/)\n",
    "* [Hugging Face on Amazon SageMaker](https://aws.amazon.com/machine-learning/hugging-face/) (and [Amazon SageMaker on Hugging Face](https://huggingface.co/docs/sagemaker/index))\n",
    "* ‚Ä¶ and [many more](https://docs.aws.amazon.com/sagemaker/latest/dg/frameworks.html)\n",
    "\n",
    "### Articles\n",
    "\n",
    "* (Moro *et al.*, 2014) A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems\n",
    "* (Sculley *et al.*, 2015) Hidden Technical Debt in Machine Learning Systems"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
